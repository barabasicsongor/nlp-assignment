<0.18.2.94.16.54.50.plp+@PHYLLIS.ADM.CS.CMU.EDU (Phyllis Pomerantz).0>
Type:     AI Seminar
Who: <speaker>Eugene Charniak</speaker>
Topic:    Improved Statistical Language Models from Syntactic Parsing
Dates:    22Feb94
Time:     <stime>3:30 PM</stime>
Place: <location>5409 Wean Hall</location>
Host:     Danny Sleator
PostedBy: plp+ on 18Feb94 at 16:54 from PHYLLIS.ADM.CS.CMU.EDU (Phyllis Pomerantz)
Abstract: 


 <paragraph>Type:     AI Seminar
 Who:      <speaker>Eugene Charniak</speaker>
 Topic:    Improved Statistical Language Models from Syntactic Parsing
 Dates:    22Feb94
 Time:     <stime>3:30 p.m</stime>.
 Place:    <location>5409 Wean Hall</location>
 Host:     Danny Sleator</paragraph>

<paragraph>Improved Statistical Language Models from Syntactic Parsing
<speaker>Eugene Charniak</speaker>
Brown University</paragraph>

<paragraph>A statistical language model assigns a probability to every sequence of
words such that common sequences in the language ("I have a headache") have
high probability and uncommon ones ("Headache a have I") have low.  Such
models are of most obvious use in speech recognition, but they have many
other uses as well.  The current ``gold standard'' in statistical language
models is the trigram model, which estimates the probability of each
successive word using statistics gathered on the probability of the word
given the last two words.  <sentence>This is very dumb, but remarkably successful. </sentence>  We
hope to create better models using more standard NLU techniques.  We hope to
model the language by first parsing the sentence, then collecting statistics
based upon the parse (not just the last few words).</paragraph>

<paragraph>In this talk we concentrate on the first of these steps and look in
particular at probabilistic contextfree grammar learning.  Our scheme
starts with a restricted form of contextfree grammar such that only a
finite number of rules apply to any given sentence.  Starting with these
rules, we then remove excess rules using the ``insideoutside'' algorithm.
<sentence>We concentrate on two interesting modifications of this scheme. </sentence>  In the
first we create several different grammars for the language using different
subsets of our training data and then merge them.  Interestingly, this
significantly improves the quality of the learned grammar.  In the second we
learn a ``pseudocontextsensitive'' grammar by collecting extra statistics
on rule application (``pseudo'' because the resulting formalism could be put
back into contextfree form by multiplying out the nonterminals of the
language).  <sentence>This too leads to significant improvements. </sentence></paragraph>